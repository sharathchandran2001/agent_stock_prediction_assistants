# !pip install torch scikit-learn matplotlib --quiet

import torch
import torch.nn as nn
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np

# -------------------------------
# Step 1: Prepare Log Sequences
# -------------------------------
log_sequences = [
    ["user login successful", "USB device connected", "application started successfully"],
    ["authentication failed", "CPU temperature warning", "memory leak detected"],
    ["cron job completed", "system reboot initiated", "kernel panic error"],
    ["cron job completed", "system reboot initiated", "kernel panic error"]
]

# Add test log sequence from uploaded image (Apache error)
log_sequences.append([
    "Apache started",
    "child process (776) exiting unexpectedly with status 3221225477",
    "server auto-recovery triggered"
])

labels = [0, 1, 1, 1, 1]  # Last one is anomaly

# -------------------------------
# Step 2: TF-IDF Vectorization
# -------------------------------
flattened = [log for seq in log_sequences for log in seq]
vectorizer = TfidfVectorizer(max_features=8)
vectorizer.fit(flattened)

def encode_sequence(seq):
    return torch.tensor(np.array([vectorizer.transform([log]).toarray()[0] for log in seq]), dtype=torch.float32)

X_seq = [encode_sequence(seq) for seq in log_sequences]
y_seq = torch.tensor(labels, dtype=torch.float32).unsqueeze(1)

# Split into train/test
X_train = torch.stack(X_seq[:-1])       # All but last
y_train = y_seq[:-1]
X_test = X_seq[-1].unsqueeze(0)         # Last one as test
y_test = y_seq[-1].unsqueeze(0)

# -------------------------------
# Step 3: Classical LSTM Model
# -------------------------------
class ClassicalLSTM(nn.Module):
    def __init__(self, input_dim, hidden_dim, seq_len):
        super().__init__()
        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, batch_first=True)
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        out, (h_n, c_n) = self.lstm(x)        # h_n: (1, batch, hidden)
        return self.classifier(h_n.squeeze(0))  # shape: (batch, 1)

# -------------------------------
# Step 4: Train Model
# -------------------------------
input_dim = X_train.shape[-1]
hidden_dim = 8
seq_len = 3

model = ClassicalLSTM(input_dim, hidden_dim, seq_len)
loss_fn = nn.BCELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

print("Training Classical LSTM...\n")
for epoch in range(30):
    model.train()
    optimizer.zero_grad()
    y_pred = model(X_train)
    loss = loss_fn(y_pred, y_train)
    loss.backward()
    optimizer.step()
    if epoch % 5 == 0:
        acc = ((y_pred > 0.5) == y_train).float().mean()
        print(f"Epoch {epoch:02d}: Loss={loss.item():.4f}  Accuracy={acc.item():.2f}")

# -------------------------------
# Step 5: Evaluation
# -------------------------------
model.eval()
with torch.no_grad():
    train_preds = (model(X_train) > 0.5).int().flatten()
    test_pred = (model(X_test) > 0.5).int().item()

    print("\nTrain Predictions :", train_preds.tolist())
    print("Train Labels      :", y_train.int().flatten().tolist())
    print("Test Prediction   :", test_pred)
    print("Test Label        :", int(y_test.item()))

    # Visualization
    plt.figure(figsize=(8, 3))
    plt.plot(y_train.flatten(), 'go', label="Train Actual")
    plt.plot(train_preds, 'rx', label="Train Predicted")
    plt.axhline(test_pred, color='r', linestyle='--', label="Test Predicted")
    plt.axhline(y_test.item(), color='g', linestyle='--', label="Test Actual")
    plt.title("Classical LSTM Anomaly Detection: Train vs Test")
    plt.xlabel("Train Sequence Index")
    plt.ylabel("Anomaly (1) or Normal (0)")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()
